{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neuron class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, num_inputs):\n",
    "        self.weights = [random.random() for _ in range(num_inputs)]\n",
    "        self.bias = random.random()\n",
    "        self.error = None\n",
    "\n",
    "    # activatie functie om de output van de neuron te berekenen\n",
    "    def activate(self, inputs):\n",
    "        total = sum(w * i for w, i in zip(self.weights, inputs))\n",
    "        x = total + self.bias\n",
    "        self.output = 1 / (1 + math.exp(-x))\n",
    "        return self.output\n",
    "    \n",
    "    # berekenen van de error van de output neuron\n",
    "    def output_error(self, target):\n",
    "        self.error = self.output * (1 - self.output)\n",
    "        self.error *= (self.output - target)\n",
    "\n",
    "    # berekenen van de error van de hidden neuron\n",
    "    def hidden_error(self, next_layer_weights, next_layer_errors):\n",
    "        self.error = 0\n",
    "        for weight, error in zip(next_layer_weights, next_layer_errors):\n",
    "            self.error += weight * error\n",
    "        self.error *= self.output * (1 - self.output)\n",
    "   \n",
    "    # berekenen van de weight gradients van de neuron\n",
    "    def compute_weight_gradient(self, prev_layer_output):\n",
    "        weight_gradients = []\n",
    "        for prev_output in prev_layer_output:\n",
    "            weight_gradient = prev_output * self.error\n",
    "            weight_gradients.append(weight_gradient)\n",
    "        return weight_gradients\n",
    "    \n",
    "    # berekenen van de bias gradient van de neuron\n",
    "    def compute_bias_gradient(self):\n",
    "        return self.error\n",
    "    \n",
    "    # berekenen van de weight delta van de neuron\n",
    "    def delta(self, learning_rate, delta_j):\n",
    "        return [learning_rate * self.output * delta_j for _ in range(len(self.weights))]\n",
    "\n",
    "    # berekenen van de bias delta van de neuron\n",
    "    def update(self, learning_rate, weight_gradients):\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= learning_rate * weight_gradients[i]\n",
    "        self.bias -= learning_rate * self.bias_gradient\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Weights: {self.weights}, Bias: {self.bias}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NeuronLayer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuronLayer:\n",
    "    def __init__(self, num_neurons, num_inputs):\n",
    "        self.neurons = [Neuron(num_inputs) for _ in range(num_neurons)]\n",
    "\n",
    "    # activatie functie om de output van de layer te berekenen\n",
    "    def activate(self, inputs):\n",
    "        activated_outputs = []\n",
    "        for neuron in self.neurons:\n",
    "            activated_outputs.append(neuron.activate(inputs))\n",
    "        return activated_outputs\n",
    "\n",
    "    def __str__(self):\n",
    "        return '\\n'.join(str(neuron) for neuron in self.neurons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NeuronNetwork class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuronNetwork:\n",
    "    def __init__(self, num_inputs, num_hidden, num_outputs, learning_rate):\n",
    "        self.hidden_layer = NeuronLayer(num_hidden, num_inputs)\n",
    "        self.output_layer = NeuronLayer(num_outputs, num_hidden)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    # activatie functie om de output van de network te berekenen\n",
    "    def activate(self, inputs):\n",
    "        hidden_outputs = self.hidden_layer.activate(inputs)\n",
    "        self.hidden_layer_outputs = hidden_outputs\n",
    "        return self.output_layer.activate(hidden_outputs)\n",
    "\n",
    "    # backpropagation functie die ze weights en biases update\n",
    "    def backpropagate(self, inputs, target):\n",
    "        outputs = self.activate(inputs)\n",
    "\n",
    "        # berekenen van de error van de output neuron\n",
    "        for i, neuron in enumerate(self.output_layer.neurons):\n",
    "            neuron.output_error(target[i])\n",
    "            neuron.weight_gradients = neuron.compute_weight_gradient(self.hidden_layer_outputs)\n",
    "            neuron.bias_gradient = neuron.compute_bias_gradient()\n",
    "\n",
    "        # berekenen van de error van de hidden neuron\n",
    "        for i, neuron in enumerate(self.hidden_layer.neurons):\n",
    "            neuron.hidden_error([neuron.weights[i] for neuron in self.output_layer.neurons],[neuron.error for neuron in self.output_layer.neurons])\n",
    "            neuron.weight_gradients = neuron.compute_weight_gradient(inputs)\n",
    "            neuron.bias_gradient = neuron.compute_bias_gradient()\n",
    "\n",
    "        # update van de weights en biases\n",
    "        for neuron in self.output_layer.neurons:\n",
    "            neuron.update(self.learning_rate, neuron.weight_gradients)\n",
    "\n",
    "        # update van de weights en biases\n",
    "        for neuron in self.hidden_layer.neurons:\n",
    "            neuron.update(self.learning_rate, neuron.weight_gradients)\n",
    "\n",
    "    # train functie die uiteindelijk verandwoordelijk is voor het trainen van het netwerk\n",
    "    def train(self, inputs, targets, max_runs=50000, min_loss=None):\n",
    "        run = 0\n",
    "        while run < max_runs:\n",
    "            total_loss = 0\n",
    "            for input_data, target in zip(inputs, targets):\n",
    "                self.inputs = input_data\n",
    "                self.backpropagate(input_data, target)\n",
    "                total_loss += self.get_loss(input_data, target)\n",
    "\n",
    "            run += 1\n",
    "            average_loss = total_loss / len(inputs)\n",
    "            \n",
    "            # line om de loss per run te printen om zo te zien om deze verbeterd en het model dus aan het leren is\n",
    "            # print(f\"Run {run} - Average Loss: {average_loss}\")\n",
    "\n",
    "            if min_loss is not None and average_loss < min_loss:\n",
    "                print(f\"minimul Loss gehaald\")\n",
    "                break\n",
    "\n",
    "    # berekend de loss van het netwerk\n",
    "    def get_loss(self, inputs, target):\n",
    "        outputs = self.activate(inputs)\n",
    "        loss = 0.5 * sum((output - target) ** 2 for output, target in zip(outputs, target))\n",
    "        return loss\n",
    "    \n",
    "    def accuracy(y_true, y_pred):\n",
    "        correct = 0\n",
    "        for t, p in zip(y_true, y_pred):\n",
    "            if t == p:\n",
    "                correct += 1\n",
    "        total = len(y_true)\n",
    "        return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AND-poort trainen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AND-poort:\n",
      "input: [0, 0], prediction: [0]\n",
      "input: [0, 1], prediction: [0]\n",
      "input: [1, 0], prediction: [0]\n",
      "input: [1, 1], prediction: [1]\n"
     ]
    }
   ],
   "source": [
    "AND = NeuronNetwork(2, 2, 1, 0.1)\n",
    "\n",
    "train_inputs_and = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "train_targets_and = [[0], [0], [0], [1]]\n",
    "\n",
    "AND.train(train_inputs_and, train_targets_and)\n",
    "\n",
    "print(\"AND-poort:\")\n",
    "for test_input in train_inputs_and:\n",
    "    prediction = AND.activate(test_input)\n",
    "    rounded_prediction = [int(round(output)) for output in prediction]\n",
    "    print(f\"input: {test_input}, prediction: {rounded_prediction}\")\n",
    "\n",
    "# goede output zou zijn:\n",
    "# [0]\n",
    "# [0]\n",
    "# [0]\n",
    "# [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XOR-poort trainen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XOR-poort:\n",
      "Input: [0, 0], Prediction: [0]\n",
      "Input: [0, 1], Prediction: [1]\n",
      "Input: [1, 0], Prediction: [1]\n",
      "Input: [1, 1], Prediction: [0]\n"
     ]
    }
   ],
   "source": [
    "XOR = NeuronNetwork(2, 2, 1, 0.1)\n",
    "\n",
    "train_inputs_xor = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "train_targets_xor = [[0], [1], [1], [0]]\n",
    "XOR.train(train_inputs_xor, train_targets_xor)\n",
    "\n",
    "print(\"XOR-poort:\")\n",
    "for test_input in train_inputs_xor:\n",
    "    prediction = XOR.activate(test_input)\n",
    "    rounded_prediction = [int(round(output)) for output in prediction]\n",
    "    print(f\"Input: {test_input}, Prediction: {rounded_prediction}\")\n",
    "\n",
    "\n",
    "# goede output zou zijn:\n",
    "# [0]\n",
    "# [1]\n",
    "# [1]\n",
    "# [0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Half adder trainen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Half adder:\n",
      "input: [0, 0], Prrediction: [0, 0]\n",
      "input: [0, 1], Prrediction: [1, 0]\n",
      "input: [1, 0], Prrediction: [1, 0]\n",
      "input: [1, 1], Prrediction: [0, 1]\n"
     ]
    }
   ],
   "source": [
    "half_adder = NeuronNetwork(2, 2, 2, 0.1)\n",
    "\n",
    "train_inputs_half_adder = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "train_targets_half_adder = [[0, 0], [1, 0], [1, 0], [0, 1]]\n",
    "\n",
    "half_adder.train(train_inputs_half_adder, train_targets_half_adder)\n",
    "\n",
    "print(\"Half adder:\")\n",
    "for test_input in train_inputs_half_adder:\n",
    "    prediction = half_adder.activate(test_input)\n",
    "    rounded_prediction = [int(round(output)) for output in prediction]\n",
    "    print(f\"input: {test_input}, Prrediction: {rounded_prediction}\")\n",
    "\n",
    "\n",
    "\n",
    "# goede output zou zijn:\n",
    "# [0, 0]\n",
    "# [1, 0]\n",
    "# [1, 0]\n",
    "# [0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainen Iria-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier laad ik de Iris dataset in. Ik verdeel de dataset in trainings- en testsets. Daarna roep ik mijn netwerk aan met het aantal invoerkenmerken, verborgen neuronen en uitvoerkenmerken. Ik train mijn netwerk met behulp van de trainingsgegevens. Na het trainen voorspel ik de labels voor de testgegevens en bereken ik de nauwkeurigheid van de voorspellingen, en print ik de accuracy uit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hidde\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y_encoded = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "num_inputs = 4\n",
    "num_hidden = 5\n",
    "num_outputs = 3\n",
    "learning_rate = 0.1\n",
    "max_runs = 1000\n",
    "\n",
    "network = NeuronNetwork(num_inputs, num_hidden, num_outputs, learning_rate)\n",
    "network.train(X_train, y_train, max_runs=max_runs)\n",
    "\n",
    "predictions = [network.activate(x) for x in X_test]\n",
    "predictions = [max(enumerate(p), key=lambda x: x[1])[0] for p in predictions]\n",
    "true_labels = [list(target).index(1) for target in y_test]\n",
    "acc = NeuronNetwork.accuracy(true_labels, predictions)\n",
    "print(f\"accuracy = {acc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainen Digit dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In deze code laad ik de digit (soms vage) cijfers en verdeel ik deze in trainingssets en testsets. Vervolgens standaardiseerr ik de gegevens om ze geschikt te maken voor training. ik configureer mijn netwerk met het aantal invoerkenmerken, het aantal verborgen neuronen en het aantal klassen als uitvoer. KWa tijd ben ik er denk ik een paar uurtjes mee bezig geweest, dit is misschien wat lang maar ik was ook nog druk wat andere dingen aan het aanpassen en dit heeft dus ook wat tijd gekost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  0.95\n"
     ]
    }
   ],
   "source": [
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "num_inputs = X_train.shape[1]\n",
    "num_hidden = 16\n",
    "num_outputs = len(set(y_train)) \n",
    "learning_rate = 0.4\n",
    "max_runs = 10\n",
    "\n",
    "network = NeuronNetwork(num_inputs, num_hidden, num_outputs, learning_rate)\n",
    "\n",
    "network.train(X_train, [([1 if i == y else 0 for i in range(num_outputs)]) for y in y_train], max_runs=max_runs, min_loss=0.01)\n",
    "predictions = [network.activate(x) for x in X_test]\n",
    "predictions = [max(enumerate(p), key=lambda x: x[1])[0] for p in predictions]\n",
    "acc = NeuronNetwork.accuracy(y_test, predictions)\n",
    "print(\"accuracy = \", acc)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
